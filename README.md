# ðŸ§  Deep Reinforcement Learning for Personalized Radiotherapy Beam Orientation

This repository contains the code and experiments for our work on **patient-specific Beam Orientation Optimization (BOO)** in head-and-neck radiotherapy using **Deep Q-Learning**. The agent selects clinically meaningful gantry angles directly from voxel-level anatomy, **without** repeated Monte Carlo dose simulation.

> **TL;DR:** Given a CT + contours, this project picks 5 beam angles in <1s that significantly improve PTV coverage vs. equiangular baselines.

**Quick links**

- ðŸ” [Problem & approach](#-overview--problem)
- ðŸ§ª [Results (100-patient test set)](#-results--100-patient-evaluation)
- âš™ï¸ [How to run evaluation](#-reproducing-results)
- ðŸ‹ï¸ [How to train from scratch](#-training-from-scratch)
- ðŸ“ [Repository structure](#-repository-structure)
- ðŸ“„ [Paper / citation](#-citation)

## ðŸ“Œ Overview / Problem

Choosing good beam orientations is critical for high-quality radiotherapy plans.  
Conventional BOO strategies (equiangular templates, simple heuristics, combinatorial solvers):

-  Are **not personalized** to anatomy
-  Become **computationally infeasible** at scale
-  Ignore **voxel-level geometry**
-  Often require repeated, slow **dose calculations**

## ðŸš€ Proposed Solution

We formulate BOO as a **sequential decision problem** and train a Deep Q-Network (DQN) to:

- Read multi-channel 2D slices: **CT + PTV + 5 OAR masks + evolving dose**
- Select **5 non-repeating gantry angles** from 36 candidates (0â€“350Â° at 10Â° resolution)
- Accumulate a **pseudo-physical dose surrogate** over time
- Balance **PTV coverage** and **OAR avoidance** via a clinically-motivated reward

The system produces **patient-adaptive beam sets in < 1 second** (CPU only).

## ðŸ“ Repository Structure

```text
Beam-Angle-Optimization-in-Radiotherapy-Using-Deep-Reinforcement-Learning/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ experiments.json      # Experiment configuration, hyperparameters, patient splits
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ strong/               # High-performing cases (good coverage + DVH)
â”‚   â”œâ”€â”€ median/               # Typical cases
â”‚   â”œâ”€â”€ failure/              # Failure modes / missed coverage
â”‚   â””â”€â”€ anomaly/              # Outliers requiring discussion
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_dqn_model.pt     # Best-performing checkpoint (saved after training)
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ summary_results.md    # Human-readable summary of evaluation
â”‚   â””â”€â”€ test_results.csv      # Numerical metrics for 100 test patients
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ repro.py              # Reproducibility utilities (seeds, deterministic setup)
â”œâ”€â”€ baselines.py              # Equiangular / heuristic / random beam baselines
â”œâ”€â”€ eval_main.py              # Evaluation script (loads model, runs baselines, saves figs/metrics)
â”œâ”€â”€ train.py                  # DQN training pipeline (env, replay buffer, logging)
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # You are here

